{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahaanirbannew/spacy-custom-NER-bird-names/blob/main/Program_fetch_tweet_based_on_search_term%2C_create_training_data%2C_ML_model_to_NER_bird_names.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Function definitions"
      ],
      "metadata": {
        "id": "B6doKl7BNvIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mZlnmywMkrsR"
      },
      "outputs": [],
      "source": [
        "def what_search_term():\n",
        "  search_term = \"indiAves\"        ## CHANGE THIS. \n",
        "  return search_term\n",
        "  \n",
        "\n",
        "import tweepy\n",
        "import time\n",
        "import pickle\n",
        "import csv\n",
        "import datetime \n",
        "from datetime import datetime \n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "import pandas as pd \n",
        "import re\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "from spacy.util import filter_spans\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "\n",
        "def connect_to_google_drive():\n",
        "  drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "def extract_model_best_from_archive():\n",
        "  file_name = \"/content/model-best.zip\"\n",
        "  with ZipFile(file_name, 'r') as zip:\n",
        "    zip.extractall(\"/content/model-best\")\n",
        "\n",
        "def is_GPU_on():\n",
        "  if tf.test.gpu_device_name() == \"\": \n",
        "    return False\n",
        "  return True \n",
        "\n",
        "def create_twitter_app_obj():\n",
        "  consumer_key = \"iPaIdR8GRI59yTJMs0Es0dIBN\"\n",
        "  consumer_secret = \"pLadg3UaLeK3yKDujRMChRN3p8hUDBOjBsuOBy8j8ERr4zz1vs\"\n",
        "  access_token = \"39085479-AabHt6bmFSbClDfUZuHjModYPAxVlOxHeMA79UyVt\"\n",
        "  access_token_secret = \"3IqXDISfqg14wzMNNn2AX4KYG9Wfkltt21QxKasE4YNnG\"\n",
        "  # Creating the authentication object\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  # Setting your access token and secret\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  # Creating the API object while passing in auth information\n",
        "\n",
        "  try:\n",
        "    api = tweepy.API(auth) \n",
        "    return api\n",
        "  except:\n",
        "    print(\"Error: Error making the Twitter Object.\")\n",
        "    return 0\n",
        "\n",
        "def get_google_drive_folder_path():\n",
        "  path = \"/content/drive/My Drive/IndiAves/\" \n",
        "  return path \n",
        "\n",
        "def get_since_id(path,search_word):\n",
        "  file = open(path+\"since_id\",'rb')\n",
        "  try:\n",
        "    since_id_data = pickle.load(file)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    return 0\n",
        "  try: \n",
        "    since_id = since_id_data[search_word]\n",
        "    return since_id\n",
        "  except:\n",
        "    print(\"Info: This looks like a new search.\")\n",
        "    return 0\n",
        "\n",
        "def load_all_birds_list(path):\n",
        "  file = open(path+\"bird_list_df\",'rb')\n",
        "  bird_list_df = pickle.load(file)\n",
        "  try: \n",
        "    return bird_list_df\n",
        "  except:\n",
        "    print(\"Error: No bird list found.\")\n",
        "    return 0\n",
        "  \n",
        "def load_training_data(path):\n",
        "  file = open(path+\"training_data\",'rb')\n",
        "  training_data = pickle.load(file)\n",
        "  try: \n",
        "    return training_data\n",
        "  except:\n",
        "    print(\"Error: No training data found.\")\n",
        "    return 0\n",
        "\n",
        "def load_nlp_model_():\n",
        "  import spacy\n",
        "  #nlp = spacy.load(\"en_core_web_sm\")\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  print(nlp.pipe_names)\n",
        "  return nlp\n",
        "\n",
        "def create_doc_bin(training_data,nlp):\n",
        "  from spacy.tokens import DocBin\n",
        "  from tqdm import tqdm\n",
        "  from spacy.util import filter_spans\n",
        "  doc_bin = DocBin()\n",
        "  for training_example  in tqdm(training_data['annotations']): \n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text) \n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        #if span is None:\n",
        "        #    print(\"Skipping entity\")\n",
        "        #else:\n",
        "        #    ents.append(span)\n",
        "        if span is not None:\n",
        "          ents.append(span)\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents \n",
        "    doc_bin.add(doc)\n",
        "  doc_bin.to_disk(\"training_data.spacy\") # save the docbin object\n",
        "\n",
        "def export_training_data(training_data,path):\n",
        "  filepath = open(path+\"training_data\",'wb') \n",
        "  pickle.dump(training_data, filepath)                     \n",
        "  filepath.close() \n",
        "  print(len(training_data[\"annotations\"]), \"records in training data.\")\n",
        "  print(\"training data is saved.\") \n",
        "\n",
        "\n",
        "def initialisation():\n",
        "  !pip install tweet-preprocessor\n",
        "  !python -m spacy download en_core_web_sm\n",
        "  !pip install spacy[transformers]\n",
        "\n",
        "def import_base_config_file(google_drive_folder_path):\n",
        "  shutil.copyfile(google_drive_folder_path+\"base_config.cfg\", \"/content/base_config.cfg\")\n",
        "\n",
        "def basic_preprocess(tweet):\n",
        "  import preprocessor as p\n",
        "  p.set_options(p.OPT.EMOJI, p.OPT.MENTION, p.OPT.URL, p.OPT.SMILEY, p.OPT.NUMBER,p.OPT.HASHTAG)\n",
        "  tweet = tweet.lower()\n",
        "  tweet = tweet.replace(\"\\n\",\" \")  \n",
        "  tweet = tweet.replace(\"\\\\n\",\" \")\n",
        "  tweet = tweet.replace(\"'\",\"\")\n",
        "  tweet = tweet[1:] \n",
        "  tweet = p.clean(tweet)\n",
        "  tweet = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
        "  tweet = re.sub(r' x..', '', tweet)\n",
        "  tweet = re.sub(r' +', ' ', tweet) #' +', ' '\n",
        "  #tweet = re.sub(r' n. ', '', tweet) \n",
        "  tweet = tweet.replace(\"x9c\",\"\")\n",
        "  tweet = tweet.strip()\n",
        "  return tweet\n",
        "\n",
        "def preprocess_tweets_arr_batch(tweets):\n",
        "  new_tweets_ = []\n",
        "  for tweet in tweets:\n",
        "    tweet = basic_preprocess(tweet)\n",
        "    if len(tweet) > 1:\n",
        "      new_tweets_.append(tweet)\n",
        "  return new_tweets_\n",
        "\n",
        "def add_to_database(api, search_word, ext_path, since_id):\n",
        "  new_search = \"#\" +search_word + \" -filter:retweets\" \n",
        "  count = 0\n",
        "  csvFile = open(ext_path+search_word+'.csv', 'a')\n",
        "  csvFile2 = open('/content/temp_.csv', 'w') \n",
        "\n",
        "  csvWriter = csv.writer(csvFile)\n",
        "  csvWriter2 = csv.writer(csvFile2)\n",
        "\n",
        "  for tweet in tweepy.Cursor(api.search,q=new_search,count=100,\n",
        "                            lang=\"en\",\n",
        "                            since_id=since_id, tweet_mode=\"extended\").items(): \n",
        "    if tweet.id > since_id:\n",
        "      since_id = tweet.id\n",
        "    \n",
        "    media_url=\"\"\n",
        "    if tweet.entities.get('media', []): media_url = tweet.entities.get('media', [])[0]['media_url']\n",
        "\n",
        "    hashtags = []\n",
        "    for tag in tweet.entities[\"hashtags\"]:\n",
        "      hashtags.append(tag[\"text\"]) \n",
        "    \n",
        "    csvWriter.writerow([tweet.created_at, tweet.id, tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8'), tweet.full_text.encode('utf-8'), media_url, hashtags])\n",
        "    csvWriter2.writerow([tweet.created_at, tweet.id, tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8'), tweet.full_text.encode('utf-8'), media_url, hashtags])\n",
        "    count +=1\n",
        "  print(datetime.now(),count, \"tweets retrieved.\")\n",
        "  return since_id \n",
        "\n",
        "def update_since_id(since_id, path,search_word):\n",
        "  filepath = open(path+\"since_id\",'rb')\n",
        "  since_id_data = pickle.load(filepath)\n",
        "  since_id_data[search_word]=since_id\n",
        "  filepath = open(path+\"since_id\",'wb') \n",
        "  pickle.dump(since_id_data, filepath)                     \n",
        "  filepath.close()\n",
        "\n",
        "def train_model():\n",
        "  !python -m spacy init fill-config base_config.cfg config.cfg\n",
        "  !python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0\n",
        "\n",
        "def import_best_model(google_drive_folder_path):\n",
        "  shutil.copyfile(google_drive_folder_path+\"model-best.zip\", \"/content/model-best.zip\")\n",
        "\n",
        "def archive_best_model_in_google_drive(google_folder_path):\n",
        "  shutil.make_archive(google_folder_path+\"model-best\", 'zip', \"/content/model-best\")\n",
        "\n",
        "def train_and_load_best_model(training_data,nlp,google_drive_folder_path): \n",
        "  is_nlp_ready = True \n",
        "\n",
        "  try:\n",
        "    is_GPU_on_colab = is_GPU_on() \n",
        "    print(\"is_GPU_on\",is_GPU_on_colab)\n",
        "    if is_GPU_on_colab == True: \n",
        "      create_doc_bin(training_data,nlp)\n",
        "\n",
        "      #train the model \n",
        "      try:\n",
        "        import_base_config_file(google_drive_folder_path)\n",
        "        print(\"Base Config File fetched.\")\n",
        "      except Exception as e: \n",
        "        print(str(e))\n",
        "        print(\"Error: Base Config File is NOT fetched.\")\n",
        "\n",
        "      #trains the model.\n",
        "      try:\n",
        "        train_model()\n",
        "        #archive the best model to Google Drive. \n",
        "        archive_best_model_in_google_drive(google_drive_folder_path)\n",
        "      except Exception as e:\n",
        "        print(str(e))\n",
        "        print(\"Error: Model training failed.\")\n",
        "        is_GPU_on_colab = False #so that it fetches the last saved model. \n",
        "    \n",
        "    if is_GPU_on_colab == False: \n",
        "      import_best_model(google_drive_folder_path)\n",
        "      extract_model_best_from_archive()\n",
        "    \n",
        "    #load the best model\n",
        "    nlp_ner = spacy.load(\"model-best\") \n",
        "  except Exception as e:\n",
        "    return False, \"\"\n",
        "\n",
        "  return is_nlp_ready, nlp_ner\n",
        "\n",
        "\n",
        "def load_best_model_from_archive(google_drive_folder_path):\n",
        "  import_best_model(google_drive_folder_path)\n",
        "  extract_model_best_from_archive()\n",
        "  nlp_ner = spacy.load(\"model-best\") \n",
        "  return True, nlp_ner\n",
        "\n",
        "def update_training_data(training_data,new_tweets,all_birds_name,is_nlp_ready,nlp_ner):\n",
        "  for tweet in new_tweets:\n",
        "    tweet_bird_found = False \n",
        "    print(\"------------***********\") \n",
        "    print(tweet) \n",
        "    print(\"-Labelling work:\")\n",
        "    for bird in all_birds_name:\n",
        "      if tweet.find(bird.strip())>-1:\n",
        "        print(\"--From rule matching:\")\n",
        "        print(bird, tweet.find(bird), tweet.find(bird)+len(bird))\n",
        "        training_data[\"annotations\"].append({'text': tweet, 'entities': [(tweet.find(bird), tweet.find(bird)+len(bird), 'BIRDNAME')]})\n",
        "        tweet_bird_found = True  \n",
        "        \n",
        "    \n",
        "    if tweet_bird_found == False and is_nlp_ready == True:\n",
        "        doc = nlp_ner(tweet)\n",
        "        suggestion = str(doc.ents).replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\")\n",
        "        print(\"--Suggestion from NER model: \"+suggestion)\n",
        "        birds_input = input(\"Birds,:\").split(\",\")\n",
        "        if birds_input != \"-\":\n",
        "          for bird_ in birds_input: \n",
        "            bird_ = bird_.strip()\n",
        "            print(bird_, tweet.find(bird_),tweet.find(bird_)+len(bird_))\n",
        "            training_data[\"annotations\"].append({'text': tweet, 'entities': [(tweet.find(bird_), tweet.find(bird_)+len(bird_), 'BIRDNAME')]})\n",
        "  return training_data \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awapYvyufDqi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Program - 1\n",
        "\n",
        "Run this only once.\n",
        "\n",
        "Note:\n",
        "\n",
        "\n",
        "*   Please use GPU to train model\n",
        "*   If GPU is not available, then last best model would be loaded. \n",
        "*   Assumption: All files would be available in the folder \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b4uC4koxN0y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initialisation() \n",
        "import spacy \n",
        "\n",
        "#what is the search term?\n",
        "search_word = what_search_term() \n",
        "print(search_word)\n",
        "\n",
        "#connect to google drive\n",
        "connect_to_google_drive()\n",
        "\n",
        "#connect to twitter api. \n",
        "twitter_api_obj = create_twitter_app_obj() \n",
        "\n",
        "#get Google Drive folder path\n",
        "google_drive_folder_path = get_google_drive_folder_path()  \n",
        "\n",
        "#get since_id based on the search term\n",
        "since_id = get_since_id(google_drive_folder_path,search_word) \n",
        "#print(since_id)\n",
        "\n",
        "#load all birds list \n",
        "all_birds = load_all_birds_list(google_drive_folder_path)\n",
        "\n",
        "#load training data\n",
        "training_data = load_training_data(google_drive_folder_path) \n",
        "\n",
        "#Set up Model Training. \n",
        "# PS: GPU needed. \n",
        "nlp = load_nlp_model_() #this is spacy stuff. \n",
        "\n",
        "is_nlp_ready,nlp_ner = load_best_model_from_archive(google_drive_folder_path) #for quicker work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_fTKxm6tU6K",
        "outputId": "d6684771-daa0-464d-988d-ea3f17a1080c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 37.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.6.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.4.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (8.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.64.0)\n",
            "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.1.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[transformers]) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[transformers]) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.10)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.8.5)\n",
            "Requirement already satisfied: transformers<4.22.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.21.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.12.1+cu113)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[transformers]) (0.7.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.9.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.12.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[transformers]) (2.0.1)\n",
            "indiAves\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Program - 2\n",
        "\n",
        "Keep re-running this!"
      ],
      "metadata": {
        "id": "jzz4OUx3OApL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch tweets\n",
        "since_id = add_to_database(twitter_api_obj,search_word, google_drive_folder_path, since_id) \n",
        "new_tweets_df = pd.read_csv(\"/content/temp_.csv\", names=[\"created_at\", \"tweet_id\", \"user\", \"location\", \"tweet\", \"media_url\", \"hashtags\"])\n",
        "new_tweets = preprocess_tweets_arr_batch(new_tweets_df[\"tweet\"].tolist())\n",
        "all_birds_name = all_birds[\"bird_name\"].tolist() \n",
        "\n",
        "#Update training data. ###NEEDS USER INTERVENTION. \n",
        "update_training_data(training_data,new_tweets,all_birds_name,is_nlp_ready,nlp_ner) \n",
        "\n",
        "#Exports training data \n",
        "export_training_data(training_data,google_drive_folder_path)\n",
        "\n",
        "#Update since_id\n",
        "update_since_id(since_id,google_drive_folder_path,search_word)\n",
        "\n",
        "#delete temp_.csv \n",
        "os.remove(\"/content/temp_.csv\")\n",
        "\n",
        "#copy the training data to baba's folder so that I can train the model.\n",
        "shutil.copyfile(\"/content/drive/My Drive/IndiAves/training_data\", \"/content/drive/My Drive/IndiAves_baba/training_data\")\n",
        "\n",
        "\n",
        "#Train the model and save the best model.-\n",
        "is_nlp_ready,nlp_ner = train_and_load_best_model(training_data,nlp,google_drive_folder_path)"
      ],
      "metadata": {
        "id": "CbuzT4gaN_tg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171d0988-97dc-47c5-eee4-e3f871b1751e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-05 05:21:53.007916 5 tweets retrieved.\n",
            "------------***********\n",
            "yes i agree and all the contributors who are indirect teachers of interest\n",
            "-Labelling work:\n",
            "--Suggestion from NER model: \n",
            "Birds,:-\n",
            "- -1 0\n",
            "------------***********\n",
            "eurasian curlew as they call with sound curloo oo so called curlew in latin there bill also referred as new moon due to its crescent shape amp also as archery bow\n",
            "-Labelling work:\n",
            "--From rule matching:\n",
            "eurasian curlew 0 15\n",
            "------------***********\n",
            "sep 12 ist sternocera sternocera is a genus of jewel beetles belonging to the julodinae subfamily happy teacher s day to all\n",
            "-Labelling work:\n",
            "--Suggestion from NER model: \n",
            "Birds,:-\n",
            "- -1 0\n",
            "------------***********\n",
            "red rumped swallow they normally nest under cliff overhangs in their mountain homes but will readily adapt to buildings such as mosques and bridges\n",
            "-Labelling work:\n",
            "--From rule matching:\n",
            "red rumped swallow 0 18\n",
            "------------***********\n",
            "the best whistler in town busy building its nest\n",
            "-Labelling work:\n",
            "--Suggestion from NER model: best whistler\n",
            "Birds,:-\n",
            "- -1 0\n",
            "2746 records in training data.\n",
            "training data is saved.\n",
            "is_GPU_on True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2746/2746 [00:00<00:00, 4296.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Config File fetched.\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-09-05 05:23:22,930] [INFO] Set up nlp object from config\n",
            "[2022-09-05 05:23:22,941] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-09-05 05:23:22,945] [INFO] Created vocabulary\n",
            "[2022-09-05 05:23:22,945] [INFO] Finished initializing nlp object\n",
            "[2022-09-05 05:23:24,570] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     46.17    7.55    4.53   22.61    0.08\n",
            "  0     200         66.21   2001.04   74.20   72.81   75.64    0.74\n",
            "  1     400         58.09    996.74   81.75   80.66   82.86    0.82\n",
            "  2     600         64.46   1001.08   88.65   88.17   89.15    0.89\n",
            "  3     800         78.35    900.82   91.11   89.03   93.29    0.91\n",
            "  4    1000        113.17    889.52   91.91   91.35   92.48    0.92\n",
            "  6    1200         81.68    978.67   92.70   93.94   91.50    0.93\n",
            "  7    1400         87.39   1020.68   92.55   91.69   93.42    0.93\n",
            " 10    1600         96.32   1107.44   92.05   88.83   95.51    0.92\n",
            " 13    1800        126.17   1295.31   93.15   94.15   92.18    0.93\n",
            " 16    2000        124.75   1475.88   93.10   95.88   90.47    0.93\n",
            " 21    2200        142.79   1691.21   93.27   92.70   93.85    0.93\n",
            " 26    2400        117.63   1948.31   92.97   98.38   88.12    0.93\n",
            " 31    2600        120.70   1941.08   93.54   92.45   94.66    0.94\n",
            " 37    2800        117.41   1930.28   93.39   91.79   95.04    0.93\n",
            " 42    3000        134.65   1930.83   93.27   92.41   94.15    0.93\n",
            " 48    3200        153.89   1883.55   93.40   95.61   91.28    0.93\n",
            " 53    3400         80.22   1906.74   93.19   96.82   89.83    0.93\n",
            " 58    3600        121.30   1890.42   93.47   94.65   92.31    0.93\n",
            " 64    3800        127.90   1872.88   93.09   97.61   88.97    0.93\n",
            " 69    4000        120.02   1850.26   93.27   95.81   90.85    0.93\n",
            " 75    4200        215.95   1878.12   93.48   93.24   93.72    0.93\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enter singular training data, NOT from tweets."
      ],
      "metadata": {
        "id": "JZZZ8ZIyBTgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_singular_training_data(sentence, training_data): \n",
        "  api_url = \"https://bird-name-ner-nlp.herokuapp.com/ner?sent=\"+sentence\n",
        "  response = requests.get(api_url).json()\n",
        "  print(response['bird-ner']) \n",
        "  print(response['bird-rule']) \n",
        "  birds_input = input(\"Birds,:\").split(\",\")\n",
        "  if birds_input != \"-\":\n",
        "    for bird_ in birds_input: \n",
        "      bird_ = bird_.strip()\n",
        "      print(bird_, sentence.find(bird_),sentence.find(bird_)+len(bird_))\n",
        "      training_data[\"annotations\"].append({'text': sentence, 'entities': [(sentence.find(bird_), sentence.find(bird_)+len(bird_), 'BIRDNAME')]})\n",
        "  \n",
        "  return training_data"
      ],
      "metadata": {
        "id": "nVr9MmfIAMuX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "#load training data\n",
        "training_data = load_training_data(google_drive_folder_path) \n",
        "training_data = add_singular_training_data(basic_preprocess(input(\"Enter Sentence: \")), training_data)\n",
        "#Exports training data \n",
        "export_training_data(training_data,google_drive_folder_path)\n",
        "#copy the training data to baba's folder so that I can train the model.\n",
        "shutil.copyfile(\"/content/drive/My Drive/IndiAves/training_data\", \"/content/drive/My Drive/IndiAves_baba/training_data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "T8ROHR5cI4Ik",
        "outputId": "ceef366a-5d77-4fe5-d874-a6df161209f2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Sentence: just chilling on sunday morning changeable hawk\n",
            "[]\n",
            "[]\n",
            "Birds,:changeable hawk\n",
            "changeable hawk 31 46\n",
            "2747 records in training data.\n",
            "training data is saved.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/IndiAves_baba/training_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stand alone test."
      ],
      "metadata": {
        "id": "vRTJG1GfOTzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"egyptian vultures adults n juveniles two in centre w darker morphs sky diving lessons\"\n",
        "doc = nlp_ner(sentence)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD3JN5VuKzsm",
        "outputId": "f96f64e8-31a5-4c00-d6b8-e521797a6aaa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(egyptian,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo of the API"
      ],
      "metadata": {
        "id": "lMUGp36b17LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "import requests\n",
        "\n",
        "sentence = \"egyptian vultures adults n juveniles two in centre w darker morphs sky diving lessons\"\n",
        "api_url = \"https://bird-name-ner-nlp.herokuapp.com/ner?sent=\"+sentence\n",
        "response = requests.get(api_url).json()\n",
        "print(response['bird-ner']) \n",
        "print(response['bird-rule']) \n",
        "print(response['error']) \n",
        "print(response['messages']) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "id": "vWXEHEACGokh",
        "outputId": "40311df2-ce9f-4d72-df55-8b140a8df0f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['egyptian']\n",
            "['egyptian vulture']\n",
            "[]\n",
            "['bird_list_df loaded', 'all birds loaded', '10975 birds list loaded.', 'user input recorded.', 'user input processed.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepping for Ensemble model\n"
      ],
      "metadata": {
        "id": "7EF1eflS35Eq"
      }
    }
  ]
}
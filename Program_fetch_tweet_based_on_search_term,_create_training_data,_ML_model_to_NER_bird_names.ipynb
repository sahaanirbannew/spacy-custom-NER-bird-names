{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahaanirbannew/spacy-custom-NER-bird-names/blob/main/Program_fetch_tweet_based_on_search_term%2C_create_training_data%2C_ML_model_to_NER_bird_names.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Function definitions"
      ],
      "metadata": {
        "id": "B6doKl7BNvIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mZlnmywMkrsR"
      },
      "outputs": [],
      "source": [
        "def what_search_term():\n",
        "  search_term = \"indiAves\"        ## CHANGE THIS. \n",
        "  return search_term\n",
        "  \n",
        "\n",
        "import tweepy\n",
        "import time\n",
        "import pickle\n",
        "import csv\n",
        "import datetime \n",
        "from datetime import datetime \n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "import pandas as pd \n",
        "import re\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "from spacy.util import filter_spans\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "\n",
        "def connect_to_google_drive():\n",
        "  drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "def extract_model_best_from_archive():\n",
        "  file_name = \"/content/model-best.zip\"\n",
        "  with ZipFile(file_name, 'r') as zip:\n",
        "    zip.extractall(\"/content/model-best\")\n",
        "\n",
        "def is_GPU_on():\n",
        "  if tf.test.gpu_device_name() == \"\": \n",
        "    return False\n",
        "  return True \n",
        "\n",
        "def create_twitter_app_obj():\n",
        "  consumer_key = \"iPaIdR8GRI59yTJMs0Es0dIBN\"\n",
        "  consumer_secret = \"pLadg3UaLeK3yKDujRMChRN3p8hUDBOjBsuOBy8j8ERr4zz1vs\"\n",
        "  access_token = \"39085479-AabHt6bmFSbClDfUZuHjModYPAxVlOxHeMA79UyVt\"\n",
        "  access_token_secret = \"3IqXDISfqg14wzMNNn2AX4KYG9Wfkltt21QxKasE4YNnG\"\n",
        "  # Creating the authentication object\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  # Setting your access token and secret\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  # Creating the API object while passing in auth information\n",
        "\n",
        "  try:\n",
        "    api = tweepy.API(auth) \n",
        "    return api\n",
        "  except:\n",
        "    print(\"Error: Error making the Twitter Object.\")\n",
        "    return 0\n",
        "\n",
        "def get_google_drive_folder_path():\n",
        "  path = \"/content/drive/My Drive/IndiAves/\" \n",
        "  return path \n",
        "\n",
        "def get_since_id(path,search_word):\n",
        "  file = open(path+\"since_id\",'rb')\n",
        "  try:\n",
        "    since_id_data = pickle.load(file)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    return 0\n",
        "  try: \n",
        "    since_id = since_id_data[search_word]\n",
        "    return since_id\n",
        "  except:\n",
        "    print(\"Info: This looks like a new search.\")\n",
        "    return 0\n",
        "\n",
        "def load_all_birds_list(path):\n",
        "  file = open(path+\"bird_list_df\",'rb')\n",
        "  bird_list_df = pickle.load(file)\n",
        "  try: \n",
        "    return bird_list_df\n",
        "  except:\n",
        "    print(\"Error: No bird list found.\")\n",
        "    return 0\n",
        "  \n",
        "def load_training_data(path):\n",
        "  file = open(path+\"training_data\",'rb')\n",
        "  training_data = pickle.load(file)\n",
        "  try: \n",
        "    return training_data\n",
        "  except:\n",
        "    print(\"Error: No training data found.\")\n",
        "    return 0\n",
        "\n",
        "def load_nlp_model_():\n",
        "  import spacy\n",
        "  #nlp = spacy.load(\"en_core_web_sm\")\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  print(nlp.pipe_names)\n",
        "  return nlp\n",
        "\n",
        "def create_doc_bin(training_data,nlp):\n",
        "  from spacy.tokens import DocBin\n",
        "  from tqdm import tqdm\n",
        "  from spacy.util import filter_spans\n",
        "  doc_bin = DocBin()\n",
        "  for training_example  in tqdm(training_data['annotations']): \n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text) \n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        #if span is None:\n",
        "        #    print(\"Skipping entity\")\n",
        "        #else:\n",
        "        #    ents.append(span)\n",
        "        if span is not None:\n",
        "          ents.append(span)\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents \n",
        "    doc_bin.add(doc)\n",
        "  doc_bin.to_disk(\"training_data.spacy\") # save the docbin object\n",
        "\n",
        "def export_training_data(training_data,path):\n",
        "  filepath = open(path+\"training_data\",'wb') \n",
        "  pickle.dump(training_data, filepath)                     \n",
        "  filepath.close() \n",
        "  print(len(training_data[\"annotations\"]), \"records in training data.\")\n",
        "  print(\"training data is saved.\") \n",
        "\n",
        "\n",
        "def initialisation():\n",
        "  !pip install tweet-preprocessor\n",
        "  !python -m spacy download en_core_web_sm\n",
        "  !pip install spacy[transformers]\n",
        "\n",
        "def import_base_config_file(google_drive_folder_path):\n",
        "  shutil.copyfile(google_drive_folder_path+\"base_config.cfg\", \"/content/base_config.cfg\")\n",
        "\n",
        "def basic_preprocess(tweet):\n",
        "  import preprocessor as p\n",
        "  p.set_options(p.OPT.EMOJI, p.OPT.MENTION, p.OPT.URL, p.OPT.SMILEY, p.OPT.NUMBER,p.OPT.HASHTAG)\n",
        "  tweet = tweet.lower()\n",
        "  tweet = tweet.replace(\"\\n\",\" \")  \n",
        "  tweet = tweet.replace(\"\\\\n\",\" \")\n",
        "  tweet = tweet.replace(\"'\",\"\")\n",
        "  tweet = tweet[1:] \n",
        "  tweet = p.clean(tweet)\n",
        "  tweet = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
        "  tweet = re.sub(r' x..', '', tweet)\n",
        "  tweet = re.sub(r' +', ' ', tweet) #' +', ' '\n",
        "  #tweet = re.sub(r' n. ', '', tweet) \n",
        "  tweet = tweet.replace(\"x9c\",\"\")\n",
        "  tweet = tweet.strip()\n",
        "  return tweet\n",
        "\n",
        "def preprocess_tweets_arr_batch(tweets):\n",
        "  new_tweets_ = []\n",
        "  for tweet in tweets:\n",
        "    tweet = basic_preprocess(tweet)\n",
        "    if len(tweet) > 1:\n",
        "      new_tweets_.append(tweet)\n",
        "  return new_tweets_\n",
        "\n",
        "def add_to_database(api, search_word, ext_path, since_id):\n",
        "  new_search = \"#\" +search_word + \" -filter:retweets\" \n",
        "  count = 0\n",
        "  csvFile = open(ext_path+search_word+'.csv', 'a')\n",
        "  csvFile2 = open('/content/temp_.csv', 'w') \n",
        "\n",
        "  csvWriter = csv.writer(csvFile)\n",
        "  csvWriter2 = csv.writer(csvFile2)\n",
        "\n",
        "  for tweet in tweepy.Cursor(api.search,q=new_search,count=100,\n",
        "                            lang=\"en\",\n",
        "                            since_id=since_id, tweet_mode=\"extended\").items(): \n",
        "    if tweet.id > since_id:\n",
        "      since_id = tweet.id\n",
        "    \n",
        "    media_url=\"\"\n",
        "    if tweet.entities.get('media', []): media_url = tweet.entities.get('media', [])[0]['media_url']\n",
        "\n",
        "    hashtags = []\n",
        "    for tag in tweet.entities[\"hashtags\"]:\n",
        "      hashtags.append(tag[\"text\"]) \n",
        "    \n",
        "    csvWriter.writerow([tweet.created_at, tweet.id, tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8'), tweet.full_text.encode('utf-8'), media_url, hashtags])\n",
        "    csvWriter2.writerow([tweet.created_at, tweet.id, tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8'), tweet.full_text.encode('utf-8'), media_url, hashtags])\n",
        "    count +=1\n",
        "  print(datetime.now(),count, \"tweets retrieved.\")\n",
        "  return since_id \n",
        "\n",
        "def update_since_id(since_id, path,search_word):\n",
        "  filepath = open(path+\"since_id\",'rb')\n",
        "  since_id_data = pickle.load(filepath)\n",
        "  since_id_data[search_word]=since_id\n",
        "  filepath = open(path+\"since_id\",'wb') \n",
        "  pickle.dump(since_id_data, filepath)                     \n",
        "  filepath.close()\n",
        "\n",
        "def train_model():\n",
        "  !python -m spacy init fill-config base_config.cfg config.cfg\n",
        "  !python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0\n",
        "\n",
        "def import_best_model(google_drive_folder_path):\n",
        "  shutil.copyfile(google_drive_folder_path+\"model-best.zip\", \"/content/model-best.zip\")\n",
        "\n",
        "def archive_best_model_in_google_drive(google_folder_path):\n",
        "  shutil.make_archive(google_folder_path+\"model-best\", 'zip', \"/content/model-best\")\n",
        "\n",
        "def train_and_load_best_model(training_data,nlp,google_drive_folder_path): \n",
        "  is_nlp_ready = True \n",
        "\n",
        "  try:\n",
        "    is_GPU_on_colab = is_GPU_on() \n",
        "    print(\"is_GPU_on\",is_GPU_on_colab)\n",
        "    if is_GPU_on_colab == True: \n",
        "      create_doc_bin(training_data,nlp)\n",
        "\n",
        "      #train the model \n",
        "      try:\n",
        "        import_base_config_file(google_drive_folder_path)\n",
        "        print(\"Base Config File fetched.\")\n",
        "      except Exception as e: \n",
        "        print(str(e))\n",
        "        print(\"Error: Base Config File is NOT fetched.\")\n",
        "\n",
        "      #trains the model.\n",
        "      try:\n",
        "        train_model()\n",
        "        #archive the best model to Google Drive. \n",
        "        archive_best_model_in_google_drive(google_drive_folder_path)\n",
        "      except Exception as e:\n",
        "        print(str(e))\n",
        "        print(\"Error: Model training failed.\")\n",
        "        is_GPU_on_colab = False #so that it fetches the last saved model. \n",
        "    \n",
        "    if is_GPU_on_colab == False: \n",
        "      import_best_model(google_drive_folder_path)\n",
        "      extract_model_best_from_archive()\n",
        "    \n",
        "    #load the best model\n",
        "    nlp_ner = spacy.load(\"model-best\") \n",
        "  except Exception as e:\n",
        "    return False, \"\"\n",
        "\n",
        "  return is_nlp_ready, nlp_ner\n",
        "\n",
        "\n",
        "def load_best_model_from_archive(google_drive_folder_path):\n",
        "  import_best_model(google_drive_folder_path)\n",
        "  extract_model_best_from_archive()\n",
        "  nlp_ner = spacy.load(\"model-best\") \n",
        "  return True, nlp_ner\n",
        "\n",
        "def update_training_data(training_data,new_tweets,all_birds_name,is_nlp_ready,nlp_ner):\n",
        "  for tweet in new_tweets:\n",
        "    tweet_bird_found = False \n",
        "    print(\"------------***********\") \n",
        "    print(tweet) \n",
        "    print(\"-Labelling work:\")\n",
        "    for bird in all_birds_name:\n",
        "      if tweet.find(bird.strip())>-1:\n",
        "        print(\"--From rule matching:\")\n",
        "        print(bird, tweet.find(bird), tweet.find(bird)+len(bird))\n",
        "        training_data[\"annotations\"].append({'text': tweet, 'entities': [(tweet.find(bird), tweet.find(bird)+len(bird), 'BIRDNAME')]})\n",
        "        tweet_bird_found = True  \n",
        "        \n",
        "    \n",
        "    if tweet_bird_found == False and is_nlp_ready == True:\n",
        "        doc = nlp_ner(tweet)\n",
        "        suggestion = str(doc.ents).replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\")\n",
        "        print(\"--Suggestion from NER model: \"+suggestion)\n",
        "        birds_input = input(\"Birds,:\").split(\",\")\n",
        "        if birds_input != \"-\":\n",
        "          for bird_ in birds_input: \n",
        "            bird_ = bird_.strip()\n",
        "            print(bird_, tweet.find(bird_),tweet.find(bird_)+len(bird_))\n",
        "            training_data[\"annotations\"].append({'text': tweet, 'entities': [(tweet.find(bird_), tweet.find(bird_)+len(bird_), 'BIRDNAME')]})\n",
        "  return training_data \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Program - 1\n",
        "\n",
        "Run this only once.\n",
        "\n",
        "Note:\n",
        "\n",
        "\n",
        "*   Please use GPU to train model\n",
        "*   If GPU is not available, then last best model would be loaded. \n",
        "*   Assumption: All files would be available in the folder \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b4uC4koxN0y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initialisation() \n",
        "import spacy \n",
        "\n",
        "#what is the search term?\n",
        "search_word = what_search_term() \n",
        "print(search_word)\n",
        "\n",
        "#connect to google drive\n",
        "connect_to_google_drive()\n",
        "\n",
        "#connect to twitter api. \n",
        "twitter_api_obj = create_twitter_app_obj() \n",
        "\n",
        "#get Google Drive folder path\n",
        "google_drive_folder_path = get_google_drive_folder_path() \n",
        "print(google_drive_folder_path)\n",
        "\n",
        "#get since_id based on the search term\n",
        "since_id = get_since_id(google_drive_folder_path,search_word) \n",
        "#print(since_id)\n",
        "\n",
        "#load all birds list \n",
        "all_birds = load_all_birds_list(google_drive_folder_path)\n",
        "\n",
        "#load training data\n",
        "training_data = load_training_data(google_drive_folder_path) \n",
        "\n",
        "#Set up Model Training. \n",
        "# PS: GPU needed. \n",
        "nlp = load_nlp_model_() #this is spacy stuff. \n",
        "\n",
        "is_nlp_ready,nlp_ner = train_and_load_best_model(training_data,nlp,google_drive_folder_path)\n",
        "is_nlp_ready,nlp_ner = load_best_model_from_archive(google_drive_folder_path) #for quicker work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_fTKxm6tU6K",
        "outputId": "c2c82761-af2e-457b-9a11-aa381ae5f288"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 31.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.64.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (8.1.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.9.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (57.4.0)\n",
            "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.1.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[transformers]) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[transformers]) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.10)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.12.1+cu113)\n",
            "Requirement already satisfied: transformers<4.22.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.21.2)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.8.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[transformers]) (0.7.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.12.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[transformers]) (2.0.1)\n",
            "indiAves\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/IndiAves/\n",
            "[]\n",
            "is_GPU_on True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2100/2100 [00:00<00:00, 4212.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Config File fetched.\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-09-01 02:17:43,404] [INFO] Set up nlp object from config\n",
            "[2022-09-01 02:17:43,415] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-09-01 02:17:43,419] [INFO] Created vocabulary\n",
            "[2022-09-01 02:17:43,420] [INFO] Finished initializing nlp object\n",
            "[2022-09-01 02:17:54,446] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     42.67    7.72    4.79   19.87    0.08\n",
            "  0     200         51.60   2108.42   74.01   69.83   78.73    0.74\n",
            "  1     400         55.93   1042.60   86.53   83.09   90.27    0.87\n",
            "  2     600         67.92    883.08   89.94   86.97   93.13    0.90\n",
            "  3     800        176.43    893.52   92.59   92.06   93.13    0.93\n",
            "  5    1000         68.91    810.54   92.71   91.91   93.53    0.93\n",
            "  7    1200         82.89    906.28   93.24   96.01   90.62    0.93\n",
            " 10    1400        114.57    997.94   93.41   95.49   91.42    0.93\n",
            " 13    1600         87.35   1112.45   93.46   94.88   92.07    0.93\n",
            " 16    1800        100.94   1263.85   93.68   96.39   91.12    0.94\n",
            " 21    2000         97.25   1475.28   93.68   96.84   90.72    0.94\n",
            " 27    2200        116.25   1715.31   93.67   94.17   93.18    0.94\n",
            " 33    2400        199.65   2005.07   93.62   96.49   90.92    0.94\n",
            " 40    2600        173.00   2012.76   93.74   92.86   94.63    0.94\n",
            " 47    2800         90.57   2014.25   93.78   95.33   92.27    0.94\n",
            " 54    3000        133.78   1973.60   93.93   94.70   93.18    0.94\n",
            " 61    3200        157.38   1953.58   93.64   97.81   89.81    0.94\n",
            " 68    3400        157.31   1961.65   93.92   94.83   93.03    0.94\n",
            " 75    3600         89.43   1962.27   93.70   96.05   91.47    0.94\n",
            " 82    3800        123.20   1918.19   93.75   96.54   91.12    0.94\n",
            " 88    4000         80.36   1948.46   93.72   96.64   90.97    0.94\n",
            " 95    4200        258.11   1941.30   93.72   97.92   89.86    0.94\n",
            "102    4400        192.11   1938.51   93.91   93.84   93.98    0.94\n",
            "109    4600        295.99   1923.62   93.61   96.63   90.77    0.94\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Program - 2\n",
        "\n",
        "Keep re-running this!"
      ],
      "metadata": {
        "id": "jzz4OUx3OApL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch tweets\n",
        "since_id = add_to_database(twitter_api_obj,search_word, google_drive_folder_path, since_id) \n",
        "new_tweets_df = pd.read_csv(\"/content/temp_.csv\", names=[\"created_at\", \"tweet_id\", \"user\", \"location\", \"tweet\", \"media_url\", \"hashtags\"])\n",
        "new_tweets = preprocess_tweets_arr_batch(new_tweets_df[\"tweet\"].tolist())\n",
        "all_birds_name = all_birds[\"bird_name\"].tolist() \n",
        "\n",
        "#Update training data. ###NEEDS USER INTERVENTION. \n",
        "update_training_data(training_data,new_tweets,all_birds_name,is_nlp_ready,nlp_ner) \n",
        "\n",
        "#Exports training data \n",
        "export_training_data(training_data,google_drive_folder_path)\n",
        "\n",
        "#Update since_id\n",
        "update_since_id(since_id,google_drive_folder_path,search_word)\n",
        "\n",
        "#delete temp_.csv \n",
        "os.remove(\"/content/temp_.csv\")\n",
        "\n",
        "#copy the training data to baba's folder so that I can train the model.\n",
        "shutil.copyfile(\"/content/drive/My Drive/IndiAves/training_data\", \"/content/drive/My Drive/IndiAves_baba/training_data\")"
      ],
      "metadata": {
        "id": "CbuzT4gaN_tg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4d9e802a-1f47-40d6-d333-e36b81f5c51b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-01 02:28:11.557147 0 tweets retrieved.\n",
            "2102 records in training data.\n",
            "training data is saved.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/IndiAves_baba/training_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stand alone test."
      ],
      "metadata": {
        "id": "vRTJG1GfOTzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"this is a preprocessed text and it should find spotted dove\"\n",
        "doc = nlp_ner(sentence)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD3JN5VuKzsm",
        "outputId": "276a3671-6616-4d75-8863-490ffea92b8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(spotted dove,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Labelling yesterdays data**"
      ],
      "metadata": {
        "id": "YbWhfOU24z0B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hzp1Itrb5rtO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
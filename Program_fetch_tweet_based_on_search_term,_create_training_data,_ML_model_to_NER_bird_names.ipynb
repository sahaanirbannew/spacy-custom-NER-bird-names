{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Program - fetch tweet based on search term, create training data, ML model to NER bird names.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVfbO9GSSj3k+dgOLXm/d5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahaanirbannew/spacy-custom-NER-bird-names/blob/main/Program_fetch_tweet_based_on_search_term%2C_create_training_data%2C_ML_model_to_NER_bird_names.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Function definitions"
      ],
      "metadata": {
        "id": "B6doKl7BNvIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mZlnmywMkrsR"
      },
      "outputs": [],
      "source": [
        "def what_search_term():\n",
        "  search_term = \"indiAves\"        ## CHANGE THIS. \n",
        "  return search_term\n",
        "  \n",
        "\n",
        "import tweepy\n",
        "import time\n",
        "import pickle\n",
        "import csv\n",
        "import datetime \n",
        "from datetime import datetime \n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "import pandas as pd \n",
        "import re\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "from spacy.util import filter_spans\n",
        "import shutil\n",
        "\n",
        "def connect_to_google_drive():\n",
        "  drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "\n",
        "def create_twitter_app_obj():\n",
        "  consumer_key = \"get_your_consumer_key\"\n",
        "  consumer_secret = \"get_your_consumer_secret\"\n",
        "  access_token = \"39085479-get_your_access_token\"\n",
        "  access_token_secret = \"get_your_token_secret\"\n",
        "  # Creating the authentication object\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  # Setting your access token and secret\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  # Creating the API object while passing in auth information\n",
        "\n",
        "  try:\n",
        "    api = tweepy.API(auth) \n",
        "    return api\n",
        "  except:\n",
        "    print(\"Error: Error making the Twitter Object.\")\n",
        "    return 0\n",
        "\n",
        "def get_google_drive_folder_path():\n",
        "  path = \"/content/drive/My Drive/IndiAves/\" \n",
        "  return path \n",
        "\n",
        "def get_since_id(path,search_word):\n",
        "  file = open(path+\"since_id\",'rb')\n",
        "  since_id_data = pickle.load(file)\n",
        "  try: \n",
        "    since_id = since_id_data[search_word]\n",
        "    return since_id\n",
        "  except:\n",
        "    print(\"Info: This looks like a new search.\")\n",
        "    return 0\n",
        "\n",
        "def load_all_birds_list(path):\n",
        "  file = open(path+\"bird_list_df\",'rb')\n",
        "  bird_list_df = pickle.load(file)\n",
        "  try: \n",
        "    return bird_list_df\n",
        "  except:\n",
        "    print(\"Error: No bird list found.\")\n",
        "    return 0\n",
        "  \n",
        "def load_training_data(path):\n",
        "  file = open(path+\"training_data\",'rb')\n",
        "  training_data = pickle.load(file)\n",
        "  try: \n",
        "    return training_data\n",
        "  except:\n",
        "    print(\"Error: No training data found.\")\n",
        "    return 0\n",
        "\n",
        "def load_nlp_model_():\n",
        "  import spacy\n",
        "  #nlp = spacy.load(\"en_core_web_sm\")\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  print(nlp.pipe_names)\n",
        "  return nlp\n",
        "\n",
        "def create_doc_bin(training_data,nlp):\n",
        "  from spacy.tokens import DocBin\n",
        "  from tqdm import tqdm\n",
        "  from spacy.util import filter_spans\n",
        "  doc_bin = DocBin()\n",
        "  for training_example  in tqdm(training_data['annotations']): \n",
        "    text = training_example['text']\n",
        "    labels = training_example['entities']\n",
        "    doc = nlp.make_doc(text) \n",
        "    ents = []\n",
        "    for start, end, label in labels:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        #if span is None:\n",
        "        #    print(\"Skipping entity\")\n",
        "        #else:\n",
        "        #    ents.append(span)\n",
        "        if span is not None:\n",
        "          ents.append(span)\n",
        "    filtered_ents = filter_spans(ents)\n",
        "    doc.ents = filtered_ents \n",
        "    doc_bin.add(doc)\n",
        "  doc_bin.to_disk(\"training_data.spacy\") # save the docbin object\n",
        "\n",
        "def export_training_data(training_data,path):\n",
        "  filepath = open(path+\"training_data\",'wb') \n",
        "  pickle.dump(training_data, filepath)                     \n",
        "  filepath.close()\n",
        "  print(len(training_data), \"examples of training data saved.\") \n",
        "\n",
        "def initialisation():\n",
        "  !pip install tweet-preprocessor\n",
        "  !python -m spacy download en_core_web_sm\n",
        "  !pip install spacy[transformers]\n",
        "\n",
        "def import_base_config_file():\n",
        "  shutil.copyfile(google_drive_folder_path+\"base_config.cfg\", \"/content/base_config.cfg\")\n",
        "\n",
        "def update_training_data(training_data,new_tweets,all_birds_name):\n",
        "  for tweet in new_tweets:\n",
        "    for bird in all_birds_name:\n",
        "      if tweet.find(bird.strip())>-1:\n",
        "        print(tweet)\n",
        "        print(bird, tweet.find(bird), tweet.find(bird)+len(bird))\n",
        "        training_data[\"annotations\"].append({'text': tweet, 'entities': [(tweet.find(bird), tweet.find(bird)+len(bird), 'BIRDNAME')]})\n",
        "        print(\"---\")\n",
        "      if tweet.find(bird.strip())>-1:\n",
        "        doc = nlp_ner(tweet)\n",
        "        print(tweet)\n",
        "        suggestion = str(doc.ents).replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\")\n",
        "        print(\"Suggestion: \"+suggestion)\n",
        "        birds_input = input(\"Birds,:\").split(\",\")\n",
        "        for bird_ in birds_input: \n",
        "          bird_ = bird_.strip()\n",
        "          print(bird_, tweet.find(bird_),tweet.find(bird_)+len(bird_))\n",
        "          training_data[\"annotations\"].append({'text': tweet, 'entities': [(tweet.find(bird_), tweet.find(bird_)+len(bird_), 'BIRDNAME')]})\n",
        "          print(\"---\")\n",
        "  return training_data \n",
        "\n",
        "def basic_preprocess(tweet):\n",
        "  import preprocessor as p\n",
        "  p.set_options(p.OPT.EMOJI, p.OPT.MENTION, p.OPT.URL, p.OPT.SMILEY, p.OPT.NUMBER,p.OPT.HASHTAG)\n",
        "  tweet = tweet.lower()\n",
        "  tweet = tweet.replace(\"\\n\",\" \")  \n",
        "  tweet = tweet.replace(\"\\\\n\",\" \")\n",
        "  tweet = tweet.replace(\"'\",\"\")\n",
        "  tweet = tweet[1:] \n",
        "  tweet = p.clean(tweet)\n",
        "  tweet = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
        "  tweet = re.sub(r' x..', '', tweet)\n",
        "  tweet = re.sub(r' +', ' ', tweet) #' +', ' '\n",
        "  #tweet = re.sub(r' n. ', '', tweet) \n",
        "  tweet = tweet.replace(\"x9c\",\"\")\n",
        "  tweet = tweet.strip()\n",
        "  return tweet\n",
        "\n",
        "def preprocess_tweets_arr_batch(tweets):\n",
        "  new_tweets_ = []\n",
        "  for tweet in tweets:\n",
        "    tweet = basic_preprocess(tweet)\n",
        "    if len(tweet) > 1:\n",
        "      new_tweets_.append(tweet)\n",
        "  return new_tweets_\n",
        "\n",
        "def add_to_database(api, search_word, ext_path, since_id):\n",
        "  new_search = \"#\" +search_word + \" -filter:retweets\" \n",
        "  count = 0\n",
        "  csvFile = open(ext_path+search_word+'.csv', 'a')\n",
        "  csvFile2 = open('/content/temp_.csv', 'w') \n",
        "\n",
        "  csvWriter = csv.writer(csvFile)\n",
        "  csvWriter2 = csv.writer(csvFile2)\n",
        "\n",
        "  for tweet in tweepy.Cursor(api.search,q=new_search,count=100,\n",
        "                            lang=\"en\",\n",
        "                            since_id=since_id, tweet_mode=\"extended\").items(): \n",
        "    if tweet.id > since_id:\n",
        "      since_id = tweet.id\n",
        "    \n",
        "    media_url=\"\"\n",
        "    if tweet.entities.get('media', []): media_url = tweet.entities.get('media', [])[0]['media_url']\n",
        "\n",
        "    hashtags = []\n",
        "    for tag in tweet.entities[\"hashtags\"]:\n",
        "      hashtags.append(tag[\"text\"]) \n",
        "    \n",
        "    csvWriter.writerow([tweet.created_at, tweet.id, tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8'), tweet.full_text.encode('utf-8'), media_url, hashtags])\n",
        "    csvWriter2.writerow([tweet.created_at, tweet.id, tweet.user.screen_name.encode('utf-8'), tweet.user.location.encode('utf-8'), tweet.full_text.encode('utf-8'), media_url, hashtags])\n",
        "    count +=1\n",
        "  print(datetime.now(),count, \"tweets retrieved.\")\n",
        "  return since_id \n",
        "\n",
        "def update_since_id(since_id, path,search_word):\n",
        "  filepath = open(path+\"since_id\",'rb')\n",
        "  since_id_data = pickle.load(filepath)\n",
        "  since_id_data[search_word]=since_id\n",
        "  filepath = open(path+\"since_id\",'wb') \n",
        "  pickle.dump(since_id_data, filepath)                     \n",
        "  filepath.close()\n",
        "\n",
        "def train_model():\n",
        "  !python -m spacy init fill-config base_config.cfg config.cfg\n",
        "  !python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Program - 1\n",
        "\n",
        "Run this only once.\n",
        "\n",
        "Note:\n",
        "\n",
        "\n",
        "*   Make sure you've chosen to use GPU.\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "b4uC4koxN0y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initialisation() \n",
        "import spacy \n",
        "\n",
        "#what is the search term?\n",
        "search_word = what_search_term() \n",
        "print(search_word)\n",
        "\n",
        "#connect to google drive\n",
        "connect_to_google_drive()\n",
        "\n",
        "#connect to twitter api. \n",
        "twitter_api_obj = create_twitter_app_obj() \n",
        "\n",
        "#get Google Drive folder path\n",
        "google_drive_folder_path = get_google_drive_folder_path() \n",
        "print(google_drive_folder_path)\n",
        "\n",
        "#get since_id based on the search term\n",
        "since_id = get_since_id(google_drive_folder_path,search_word) \n",
        "print(since_id)\n",
        "\n",
        "#load all birds list \n",
        "all_birds = load_all_birds_list(google_drive_folder_path)\n",
        "\n",
        "#load training data\n",
        "training_data = load_training_data(google_drive_folder_path) \n",
        "\n",
        "#Set up Model Training. \n",
        "# PS: GPU needed. \n",
        "nlp = load_nlp_model_() \n",
        "create_doc_bin(training_data,nlp)\n",
        "\n",
        "#train the model \n",
        "import_base_config_file()\n",
        "train_model()\n",
        "\n",
        "#load the best model\n",
        "nlp_ner = spacy.load(\"model-best\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_fTKxm6tU6K",
        "outputId": "d725646e-6daf-4351-f144-a7637b2486aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.4.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (8.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.64.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.9.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (4.1.1)\n",
            "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]) (1.1.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[transformers]) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[transformers]) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[transformers]) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.24.3)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.8.5)\n",
            "Requirement already satisfied: transformers<4.22.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.21.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.12.1+cu113)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy[transformers]) (0.7.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.9.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (6.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[transformers]) (2.0.1)\n",
            "indiAves\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/IndiAves/\n",
            "1564523019879739392\n",
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1866/1866 [00:00<00:00, 3602.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-08-30 19:28:05,895] [INFO] Set up nlp object from config\n",
            "[2022-08-30 19:28:05,906] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-08-30 19:28:05,910] [INFO] Created vocabulary\n",
            "[2022-08-30 19:28:05,911] [INFO] Finished initializing nlp object\n",
            "[2022-08-30 19:28:07,475] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     33.50    5.80    5.92    5.69    0.06\n",
            "  0     200         52.07   2128.55   75.99   74.68   77.34    0.76\n",
            "  1     400         60.05   1043.60   87.40   84.96   89.98    0.87\n",
            "  2     600         62.54    833.74   90.85   91.23   90.48    0.91\n",
            "  4     800         60.83    798.87   92.04   94.31   89.87    0.92\n",
            "  6    1000         79.78    882.49   93.18   92.83   93.54    0.93\n",
            "  8    1200        107.31    861.59   93.29   94.15   92.45    0.93\n",
            " 11    1400         82.99    994.62   93.19   93.45   92.94    0.93\n",
            " 14    1600         91.60   1111.53   93.29   93.03   93.54    0.93\n",
            " 18    1800        101.03   1252.97   93.63   96.46   90.97    0.94\n",
            " 23    2000        111.39   1507.77   93.48   96.56   90.59    0.93\n",
            " 30    2200        112.79   1688.25   93.81   91.87   95.84    0.94\n",
            " 37    2400         94.07   2008.61   93.78   94.75   92.83    0.94\n",
            " 45    2600        108.87   2003.30   93.92   96.21   91.74    0.94\n",
            " 52    2800        104.26   2046.04   93.99   94.72   93.27    0.94\n",
            " 60    3000         83.61   1971.21   93.72   98.97   89.00    0.94\n",
            " 68    3200         83.92   1982.92   93.96   92.25   95.73    0.94\n",
            " 75    3400        128.17   2001.79   93.78   97.52   90.31    0.94\n",
            " 83    3600        118.64   1972.96   93.73   98.73   89.22    0.94\n",
            " 91    3800         81.82   1972.29   93.81   91.47   96.28    0.94\n",
            " 99    4000        123.59   1978.93   93.78   92.63   94.96    0.94\n",
            "106    4200         83.31   1956.53   94.09   93.61   94.58    0.94\n",
            "114    4400         89.00   1934.60   93.99   93.79   94.20    0.94\n",
            "122    4600        117.17   1956.61   93.74   97.18   90.53    0.94\n",
            "129    4800        107.60   1959.20   93.90   92.97   94.85    0.94\n",
            "137    5000         86.27   1912.69   93.83   97.18   90.70    0.94\n",
            "145    5200         93.22   1921.16   93.79   96.36   91.35    0.94\n",
            "152    5400        137.42   1929.73   93.97   94.92   93.05    0.94\n",
            "160    5600        129.08   1961.16   93.94   95.90   92.06    0.94\n",
            "168    5800        111.14   1921.79   93.88   96.37   91.52    0.94\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Program - 2\n",
        "\n",
        "Keep re-running this!"
      ],
      "metadata": {
        "id": "jzz4OUx3OApL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch tweets\n",
        "since_id = get_since_id(google_drive_folder_path,search_word) #delete this.\n",
        "since_id = add_to_database(twitter_api_obj,search_word, google_drive_folder_path, since_id) \n",
        "new_tweets_df = pd.read_csv(\"/content/temp_.csv\", names=[\"created_at\", \"tweet_id\", \"user\", \"location\", \"tweet\", \"media_url\", \"hashtags\"])\n",
        "new_tweets = preprocess_tweets_arr_batch(new_tweets_df[\"tweet\"].tolist())\n",
        "all_birds_name = all_birds[\"bird_name\"].tolist() \n",
        "\n",
        "#Update training data. ###NEEDS USER INTERVENTION. \n",
        "update_training_data(training_data,new_tweets,all_birds_name)\n",
        "backup_training_data = training_data \n",
        "\n",
        "#Exports training data \n",
        "export_training_data(training_data,google_drive_folder_path)\n",
        "\n",
        "#Update since_id\n",
        "update_since_id(since_id,google_drive_folder_path,search_word)\n",
        "\n",
        "#delete temp_.csv \n",
        "os.remove(\"/content/temp_.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbuzT4gaN_tg",
        "outputId": "e76715bf-b65a-4ae3-f9bd-5791d23ad1de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-30 19:42:34.995210 0 tweets retrieved.\n",
            "2 examples of training data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stand alone test."
      ],
      "metadata": {
        "id": "vRTJG1GfOTzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"this is a preprocessed text and it should find spotted dove\"\n",
        "doc = nlp_ner(sentence)\n",
        "print(doc.ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD3JN5VuKzsm",
        "outputId": "633839dd-9031-4732-858a-cbe459dcab90"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(spotted dove,)\n"
          ]
        }
      ]
    }
  ]
}